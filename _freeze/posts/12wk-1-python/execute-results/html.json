{
  "hash": "6873ad5b2d6a258384b24eae509fefd4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '12wk-1: 최대가능도추정 (4) (Python, 참고자료)'\nauthor: 최규빈\ndate: 11/18/2025\ncrossref:\n  fig-title: \"\"\n  fig-prefix: \"\"\n---\n\n\n\n### 강의영상\n\n{{<https://youtu.be/playlist?list=PLQqh36zP38-yh7wNbew5Lx3eb7q016toA&si=mtfSTZr44OBMKB2U  >}}\n\n---\n\n### MLE 불변성(Invariance)\n\n`예제1` \n\n공평한 동전을 5회 던져서 아래와 같은 결과를 얻었다고 하자. \n\n- $0, 1, 0, 0, 1$ \n\n실패확률을 추정하라.\n\n`(풀이)` -- MLE에 기초한풀이\n\n$X_1,X_2,\\dots,X_n \\sim Ber(\\theta)$ 에서 MLE에 의한 $\\theta$의 추정값은 $\\frac{2}{5}$이므로 (왜?) 성공확률 $\\theta$는 $\\frac{2}{5}$로 추정할 수 있다. 실패할확률은 $1-\\theta$인데, $\\theta$의 값을 이미 $\\frac{2}{5}$로 추정했으므로 실패할확률의 추정값은 $1-\\frac{2}{5}=\\frac{3}{5}$이라고 할 수 있다.\n\n---\n\n`-` 개념: MLE에 기반한 추정법은 아래와 같다. \n\n1. 모수를 MLE로 추정한다 \n2. 추정의 대상과 모수와의 함수관계를 파악한다.\n3. 1에서 얻은 추정값을 2에서 얻은 함수관계에 대입한다.\n\n---\n\n\n`-` 이 논리로 위의 예제를 다시 해석하면 아래와 같다. \n\n1. 모수를 추정한다: 베르누이 분포 $Ber(\\theta)$의 모수는 성공확률 $\\theta$이며, $\\theta$의 추정값은 $\\frac{2}{5}$ 이다. \n2. 추정의 대상과 모수와의 함수관계를 파악한다: 추정의 대상은 실패확률이다. 그런데 \"$실패확률=1-\\theta$\"라는 관계가 있는데 이것이 추정의 대상과 모수와의 함수관계이다. \n3. 1에서 얻은 추정값을 2에서 얻은 함수관계에 대입한다: 우리가 1에서 얻은 추정값은 $\\frac{2}{5}$이다. 2에서 얻은 함수관계는 $$추정의대상=실패확률=1-\\theta$$이다. 1에서 얻은 $\\theta$에 대한 추정값 $\\frac{2}{5}$를 위의 $\\theta$자리에 plugin하면 $실패확률=\\frac{3}{5}$를 얻는다. \n\n---\n\n`-` 주의(고급내용): 3의 과정을 수식으로 표현하면 아래와 같다. \n\n$$\\widehat{실패확률}^{MLE}=\\widehat{1-\\theta}^{MLE}=1-\\hat{\\theta}^{MLE}$$\n\n이러한 논리전개는 언뜻 당연해보이지만 사실 당연한건 아니다. 아래와 같은 사실은 \n\n$$\\widehat{1-\\theta}^{MLE}=1-\\hat{\\theta}^{MLE}$$\n\n일반적인 추정법에는 성립하지 않는다. 오직 MLE에서만 이러한 논리전개가 가능함. (MLE가 가진 좋은 성질임)\n\n---\n\n`예제2` \n\n아래에서 \n\n$$X \\sim N(\\mu,1)$$\n\n얻은 10개의 샘플이 다음과 같다고 하자.\n\n::: {#d7cbdd51 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n[-0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n  0.46091621 -1.26506123 -0.68685285 -0.44566197]\n```\n:::\n:::\n\n\n$\\mathbb{E}(X^2)$을 추정하라.\n\n`(참고)` \n\n- 진실세계의정보: $\\mathbb{E}(X), \\mathbb{E}(X^2)$\n- 데이터세계의정보: $\\bar{x}$\n\n---\n\n`(풀이)`\n\n언뜻 생각하면 아래와 같이 추정해야 할 것 같다. \n\n::: {#73a1fefc .cell execution_count=3}\n``` {.python .cell-code}\nnp.mean(x**2)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nnp.float64(0.8243025996854358)\n```\n:::\n:::\n\n\n그런데 \n$$\\mathbb{E}(X^2)=\\big\\{\\mathbb{E}(X)\\big\\}^2+\\mathbb{V}(X)$$\n\n임을 고려하면 아래와 같이 추정하는것도 말이 되는 것 같다.\n\n::: {#aca1750f .cell execution_count=4}\n``` {.python .cell-code}\nnp.mean(x)**2 + 1\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nnp.float64(1.005568986891666)\n```\n:::\n:::\n\n\n두 값은 얼추 비슷하지만 완전히 같지는 않다. 둘다 타당한 추정법인것 같은데 무엇이 더 올바른 추정일까?\n\n---\n\n**분석**\n\n::: {#f06435a4 .cell execution_count=5}\n``` {.python .cell-code}\nA = []\nB = []\nfor i in range(1000):\n    x = np.random.randn(10)\n    A.append(np.mean(x**2))\n    B.append(1 + np.mean(x)**2)\nplt.scatter(range(1000), A, c='red', s=5, label='mean(x^2)')\nplt.scatter(range(1000), B, c='blue', s=5, label='1+mean(x)^2')\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![그림1: 추정의비교, 빨간색은 $\\text{mean}(x^2)$, 파란색은 $1+\\text{mean}(x)^2$.](12wk-1-python_files/figure-revealjs/cell-6-output-1.png){}\n:::\n:::\n\n\n---\n\n비교한 결과 방법A와 방법B중에서는 방법B가 더 우수하다는 생각이 든다. \n\n- 방법A: $\\text{mean}(x^2)=\\bar{x^2}$\n- 방법B: $1+\\text{mean}(x)^2=1+\\bar{x}^2$\n\n왜 그럴까? 사실 방법B는 최대가능도방법의 원리로 추정된 값이기 때문이다. \n\n1. 모수를 MLE로 추정한다: 정규분포의 모수는 $N(\\mu,\\sigma^2)$인데, $\\sigma=1$로 주어졌으므로 추정할 필요가 없고, $\\mu$를 추정하면 $\\bar{x}$이다. \n2. 추정의 대상과 모수와의 함수관계를 파악한다: $\\mathbb{V}(X)=\\mathbb{E}(X^2)-\\{\\mathbb{E}(X)\\}^2$의 관계가 있으므로 추정의 대상은 $\\mathbb{E}(X^2)=\\sigma^2+\\mu^2$와 같이 표현할 수 있음. 그런데 $\\sigma^2=1$이 주어졌으므로 \"$추정의대상=\\mathbb{E}(X^2)=1+\\mu^2$\" 의 관계가 있음. \n3. 1에서 얻은 추정값을 2에서 얻은 함수관계에 대입한다: 따라서 $${\\mathbb{E}(X^2)}의 추정값 = 1+\\bar{x}^2$$\n\n---\n\n### 균등분포\n\n구간 $(a,b)$에서 임의의 난수를 뽑는 실험을 상상하자. 이러한 실험은 아래와 같이 표현할 수 있다. \n$$X \\sim U(a,b)$$\n\n여기에서 $U$는 uniform의 약자이다. 히스토그램을 그려보자. (이때 $a=1, b=2$를 가정함)\n\n::: {#571c6922 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![그림2: 균등분포의 히스토그램](12wk-1-python_files/figure-revealjs/cell-7-output-1.png){}\n:::\n:::\n\n\n---\n\n히스토그램의 극한을 상상하고 pdf의 수식을 쓰면 아래와 같다.\n\n$$f_X(x)=\\frac{1}{b-a}, \\quad a \\leq x \\leq b$$\n\n수식을 좀 더 정리하면 아래와 같다.\n\n$$f_X(x)=\\frac{1}{b-a}I(a \\leq x \\leq b)$$\n\n여기에서 $I(a \\leq x \\leq b)=\\begin{cases} 1 & a \\leq x \\leq b \\\\ 0 & o.w. \\end{cases}$ 이다. 이제 아래와 같은 관측치를 얻은 상황에서 `x`의 중심 $\\mathbb{E}(X)$를 추정하는 문제를 고려하자.\n\n::: {#5fdb63b5 .cell execution_count=7}\n``` {.python .cell-code}\nx = np.array([1.287578, 1.788305, 1.408977, 1.883017, 1.940467,\n              1.045556, 1.528105, 1.892419, 1.551435, 1.456615])\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\narray([1.287578, 1.788305, 1.408977, 1.883017, 1.940467, 1.045556,\n       1.528105, 1.892419, 1.551435, 1.456615])\n```\n:::\n:::\n\n\n---\n\n`(1)` 가능도함수를 구하라.\n\n`(풀이)`\n\n가능도함수의 수식은 아래와 같다. \n$$L(\\theta|data)=f(data|\\theta)$$\n\n여기에서 $\\theta = (a,b)$ 이다. 그런데 \n\n- $f(data|\\theta) = f(x_1,\\dots,x_{10}|\\theta)=f(x_1|\\theta)\\dots f(x_{10}|\\theta)$\n- $f(x_1|\\theta) = \\frac{1}{b-a}I(a\\leq x_1 \\leq b)=\\frac{1}{b-a}I(a\\leq x_1)I(x_1\\leq b)$\n\n이므로 가능도 함수는 아래와 같다. \n\n$$L(a,b)=\\frac{1}{(b-a)^{10}}\\times I(a\\leq x_1)\\dots I(a \\leq x_n)\\times I(x_1\\leq b)\\dots I(x_n\\leq b)$$\n\n---\n\n가능도 함수를 정의하기전에\n\n::: {#c8826f16 .cell execution_count=8}\n``` {.python .cell-code}\na = 1.2\nb = 1.5\n```\n:::\n\n\n주어진 $a,b$에 대한 아래의 값들을 조사하여보자. \n\n::: {#a3ce11e5 .cell execution_count=9}\n``` {.python .cell-code}\nprint((b-a)**10)\nprint(x)\nprint(a <= x)\nprint(x <= b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5.9049000000000085e-06\n[1.287578 1.788305 1.408977 1.883017 1.940467 1.045556 1.528105 1.892419\n 1.551435 1.456615]\n[ True  True  True  True  True False  True  True  True  True]\n[ True False  True False False  True False False False  True]\n```\n:::\n:::\n\n\n가능도함수는 아래와 같이 정의된다. \n\n::: {#c798dbc0 .cell execution_count=10}\n``` {.python .cell-code}\nlikelihood = 1/(b-a)**10 * np.prod(a <= x) * np.prod(x <= b)\nlikelihood\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nnp.float64(0.0)\n```\n:::\n:::\n\n\n---\n\n`(2)` 가능도함수의 특징을 살펴보자. \n\n(풀이)\n\n- 가능도함수는 항상 양수임\n- 가능도가 0이 안나올라면 $a$는 $1.045556$보다 작아야하고, $b$는 $1.940467$보다 커야함.\n\n---\n\n`(3)` 가능도함수의 값을 표로 정리하자. \n\n(풀이)\n\n$L(a,b)$에 대한 값을 조사하여 표를 만들어보면 아래와 같다. \n\n| | $b=1.5$ | $b=1.8$ | $b=2.1$ | $b=2.4$ | $b=2.7$ | $b=3$ |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $a=0$ | 0.00e+00 | 0.00e+00 | 6.00e-04 | 1.58e-04 | 4.86e-05 | 1.69e-05 |\n| $a=0.3$ | 0.00e+00 | 0.00e+00 | 2.80e-03 | 6.00e-04 | 1.58e-04 | 4.86e-05 |\n| $a=0.6$ | 0.00e+00 | 0.00e+00 | 1.73e-02 | 2.80e-03 | 6.00e-04 | 1.58e-04 |\n| $a=0.9$ | 0.00e+00 | 0.00e+00 | 1.62e-01 | 1.73e-02 | 2.80e-03 | 6.00e-04 |\n| $a=1.2$ | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 |\n| $a=1.5$ | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 | 0.00e+00 |\n\n\n표를보면 $(a,b)=(0.9, 2.1)$에서 최대값을 가지는 듯 하다.\n\n---\n\n`(4)` 가능도함의 그림을 그려보자. \n\n(풀이)\n\n::: {#00084e64 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![그림3: $a$, $b$에 대한 likelihood 함수의 3D plot](12wk-1-python_files/figure-revealjs/cell-12-output-1.png){}\n:::\n:::\n\n\n$(a,b)=(1.1,1.9)$ 에서 최대값을 가지는 것 같다. \n\n---\n\n`(5)` 이론적인 MLE를 구하라. \n\n(풀이)\n\n가능도함수의 수식은 아래와 같다. \n$$L(a,b)=\\frac{1}{(b-a)^{10}}\\times I(a\\leq x_1)\\dots I(a \\leq x_n)\\times I(x_1\\leq b)\\dots I(x_n\\leq b)$$\n\n그런데 \n\n- $I(a\\leq x_1)\\dots I(a \\leq x_n) = I(a \\leq \\min(x_1,\\dots,x_n))$ \n- $I(x_1\\leq b)\\dots I(x_n\\leq b)= I(\\max(x_1,\\dots,x_n)\\leq b)$ \n\n이고 \n\n---\n\n::: {#b710d9fe .cell execution_count=12}\n``` {.python .cell-code}\nprint(np.min(x)); print(np.max(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.045556\n1.940467\n```\n:::\n:::\n\n\n이므로 \n\n- $I(a\\leq x_1)\\dots I(a \\leq x_n) = I(a \\leq \\min(x_1,\\dots,x_n))=I(a\\leq 1.045556)$ \n- $I(x_1\\leq b)\\dots I(x_n\\leq b)= I(\\max(x_1,\\dots,x_n)\\leq b)=I(1.940467 \\leq b)$ \n\n와 같이 정리된다. 따라서 가능도함수는 아래와 같다. \n\n$$L(a,b)=\\frac{1}{(b-a)^{10}}\\times I(a\\leq 1.045556)\\times I(1.940467 \\leq b)$$\n\n수식을 살펴보면 (1) $a\\leq 1.045556$ 이고 $b\\geq 1.940467$일 경우만 가능도함수는 $0$이 아닌 값을 가지고 (2) 그 값은 $(b-a)$값이 작을수록 최대가 되므로 $$(a,b)=(1.045556,1.940467)$$에서 이론적인 최대값을 가진다. \n\n---\n\n이 결과는 일반화될 수 있으므로 아래와 같이 정리가능하다. \n\n> $X_1,\\dots,X_n \\overset{iid}{\\sim} U(a,b)$ 일때 가능도함수는 아래와 같고 \n> $$L(a,b)=\\frac{1}{(b-a)^n}\\times I(a \\leq \\min(data)) \\times I(\\max(data)\\leq b)$$\n> $a,b$를 최대가능도추정법으로 추정하면 각각 아래와 같다. \n>\n> - $a의 추정값 = \\min(data)$\n> - $b의 추정값 = \\max(data)$\n\n---\n\n`(6)` $\\mathbb{E}(X)$의 값을 추정하라. (최대가능도추정법을 사용할 것)\n\n(풀이)\n\n아래를 다시 따르면 된다. \n\n1. 모수를 MLE로 추정한다.\n2. 추정의 대상과 모수와의 함수관계를 파악한다.\n3. 1에서 얻은 추정값을 2에서 얻은 함수관계에 대입한다. \n\n`1`. 균등분포의 모수는 $a,b$이고 이를 각각 추정한 결과는 $\\min(data)$, $\\max(data)$이다. \n\n`2`. 추정의대상은 $X$의 평균인데 이 값은 (직관적으로) $$\\mathbb{E}(X)=\\frac{a+b}{2}$$ 와 같이 계산할 수 있다. \n\n`3`. 따라서 평균의 추정값은 $\\frac{\\min(data)+\\max(data)}{2}$로 계산할 수 있다. \n\n---\n\n이제 11wk-2의 `예제3`(마지막 예제)의 결과를 다시 살펴보자. 왜 중심을 추론할때 양 극단값만 고려하면 충분한지 이해가 되는가?\n\n---\n\n`-` 결론\n\n- MLE는 데이터분석에서 가장 유용한 추정방법이라 할 수는 없지만 (사람마다 취향이 있으니까요?)\n- 꽤 유용하고 많은 분야에서 의미있게 사용된다. \n- 특히 기존에 사용되었던 MME (평균으로 해결하자는 주의) 와 비교해서는 성능이 좋다고 알려져있다.^[불편성에서만 조금 밀리고, 일치성/최소분산성등의 성질에서 동등하거나 우의에 있음]\n\n",
    "supporting": [
      "12wk-1-python_files"
    ],
    "filters": [],
    "includes": {}
  }
}